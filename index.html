<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ITA-MDT</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework <br> for Image-Based Virtual Try-On</h1>
            <div class="is-size-4 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jiwoohong93.github.io/" target="_blank">Ji Woo Hong</a>,</span>
                <span class="author-block">
                  <a href="https://triton99.github.io/" target="_blank">Tri Ton</a>,</span>
                  <span class="author-block">
                    <a href="https://trungpx.github.io/" target="_blank">Trung X. Pham</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://kookie12.github.io/" target="_blank">Gwanhyeong Koo</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://dbstjswo505.github.io/" target="_blank">Sunjae Yoon</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://sanctusfactory.com/family.php" target="_blank">Chang D. Yoo</a>
                  </span>
                  </div>

                  <div class="is-size-4 publication-authors">
                    <span class="author-block">Korea Advanced Institute of Science and Technology (KAIST)<br>CVPR 2025</span>
                    <span class="eql-cntrb"></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.20418" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
  
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.20418" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- BibTeX link -->
                  <span class="link-block">
                    <a href="https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=ITA-MDT%3A+Image-Timestep-Adaptive+Masked+Diffusion+Transformer+Framework+for+Image-Based+Virtual+Try-On&btnG=" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-book"></i>
                    </span>
                    <span>BibTeX</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jiwoohong93/ita-mdt_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_upper1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our ITA-MDT on DressCode Upper-body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_upper2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative results of our ITA-MDT on DressCode Upper-body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_lower1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our ITA-MDT on DressCode Lower-body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_lower2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative results of our ITA-MDT on DressCode Lower-body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_dresses1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results of our ITA-MDT on DressCode Dresses.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/qual_dresses2.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        More qualitative results of our ITA-MDT on DressCode Dresses.
     </h2>
   </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-quarter">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified" style="font-size: 1.2rem" >
          <p>
            This paper introduces <b>ITA-MDT</b>, the <b>Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On (IVTON)</b>, designed to overcome the limitations of previous approaches by leveraging the Masked Diffusion Transformer (MDT) for improved handling of both global garment context and fine-grained details. The IVTON task involves seamlessly superimposing a garment from one image onto a person in another, creating a realistic depiction of the person wearing the specified garment. Unlike conventional diffusion-based virtual try-on models that depend on large pre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable <font color="#e0754f"><b>transformer-based denoising diffusion model with a mask latent modeling scheme</b></font>, achieving competitive results while reducing computational overhead. A key component of ITA-MDT is the <font color="#497c27"><b>Image-Timestep Adaptive Feature Aggregator (ITAFA)</b></font>, a dynamic feature aggregator that combines all of the features from the image encoder into a unified feature of the same size, guided by diffusion timestep and garment image complexity. This enables adaptive weighting of features, allowing the model to emphasize either global information or fine-grained details based on the requirements of the denoising stage. Additionally, the <font color="#f5a100"><b>Salient Region Extractor (SRE)</b></font> module is presented to identify complex region of the garment to provide high-resolution local information to the denoising model as an additional condition alongside the global information of the full garment image. This targeted conditioning strategy enhances detail preservation of fine details in highly salient garment regions, optimizing computational resources by avoiding unnecessarily processing entire garment image. Comparative evaluations confirms that ITA-MDT improves efficiency while maintaining strong performance, reaching state-of-the-art results in several metrics.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Single image with description and fixed width -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Centered Title -->
      <div class="has-text-centered">
        <h2 class="title is-2">Overview</h2>
      </div>
      <br>
      <figure class="image" style="margin: 0 auto; max-width: 1280px;">
        <img src="static/images/overview_mdt-ivton.jpg" alt="Result Image" style="width: 100%; height: auto;">
      </figure>

      <h3 class="subtitle is-5 mt-4" >
        <p>
          <strong>Overview of the ITA-MDT Framework for Image-Based Virtual Try-On (IVTON).</strong> </br>
          The framework takes multiple reference images encoded into latent space as query, which includes 
          Garment Agnostic Map \( A \), DensePose \( P \), and Garment Agnostic Mask \( M_X \).<br>
          These reference latent images are concatenated to be patch embedded for the masked diffusion process 
          within our <font color="#e0754f"><b>MDT-IVTON</b>, which follows the architecture of MDTv2 with integrated cross-attention blocks</font>. <br>
          The image feature of Garment \( X \) is extracted with the ViT image encoder, DINOv2, 
          and is <font color="#497c27">adaptively aggregated with our proposed <b>Image-Timestep Adaptive Feature Aggregator (ITAFA)</b></font> 
          to produce Garment Feature \( F_g \). <br>
          With our <font color="#f5a100"><b>Salient Region Extractor (SRE)</b>, the Salient Region \( X_s \) is extracted from the Garment \( X \)</font> 
          and processed through ITAFA separately to produce Salient Feature \( F_s \).
          The Garment Feature \( F_g \) and Salient Feature \( F_s \) are concatenated to serve as conditions of MDT-IVTON.
        </p>
        
      </h3>
    </div>
  </div>
</section>

<!-- Single image with description and fixed width -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Centered Title -->
      <div class="has-text-centered">
        <h2 class="title is-2">ITAFA & SRE</h2>
      </div>
      <br>
      <figure class="image" style="margin: 0 auto; max-width: 1280px;">
        <img src="static/images/itafa_sre.jpg" alt="Result Image" style="width: 100%; height: auto;">
      </figure>

      <h3 class="subtitle is-5 mt-4">

          <strong>Illustration of the <font color="#497c27">Image-Timestep Adaptive Feature Aggregator (ITAFA)</font> and the <font color="#f5a100">Salient Region Extractor (SRE)</font>.</strong>
          <br>
          <strong><font color="#497c27">Left:</font></strong> The ITAFA dynamically aggregates Vision Transformer (ViT) feature embeddings <em>f</em> based on a combination of the <b>Timestep Embedding Projector</b>, which projects the diffusion timestep embedding <em>T</em> to match the feature embedding dimensions, and the <b>Image Complexity Projector</b>, which transforms the image complexity vector [<em>S, V, G</em>] (sparsity, variance, gradient magnitude) into a comparable dimension.

          The weight vectors are combined and normalized via softmax to form <em>W</em>, which is used to <b>adaptively aggregate the feature embeddings {<em>f<sub>i</sub></em> }<sub>i=0</sub><sup>h</sup> across hidden layers to produce the final output tensor <em>F</em></b>. Garment <em>X</em> and Salient Region <em>X<sub>s</sub></em> are processed separately through ITAFA to generate the Garment Feature <em>F<sub>g</sub></em> and Salient Feature <em>F<sub>s</sub></em>.
          <br>
          <strong><font color="#f5a100">Right:</font></strong> The SRE processes the input image <em>X</em> by computing an <b>entropy map <em>X<sub>e</sub></em></b>, creating a binary High-Entropy Mask <em>X<sub>m</sub></em> , and applying circular region expansion from the entropy centroid to <b>extract the final high-entropy region <em>X<sub>s</sub></em></b> , ensuring preservation of detail within a consistent aspect ratio.

        
      </h3>
    </div>
  </div>
</section>



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-2">Qualitative Comparisons</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/qual_comp_ours.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Qualitative comparison of the effect of each component of the ITA-MDT framework on VITON-HD. HR refers to the use of
            single High-resolution (448 × 448 × 3) garment image to formulate condition.
            
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/qual_comp.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Qualitative comparison of ITA-MDT with prior methods on VITON-HD and DressCode Upper-body datasets. The first three rows show VITON-HD results; the next three show DressCode results.
          </h2>
        </div>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_comp_viton1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our ITA-MDT and previous methods on the VITON-HD.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_comp_viton2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparison between our ITA-MDT and previous methods on the VITON-HD.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_comp_upper1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our ITA-MDT and previous methods on the DressCode Upper-body.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual_comp_upper2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          More qualitative comparison between our ITA-MDT and previous methods on the DressCode Upper-body.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!--Citation -->
  <section class="section" id="Citation">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>
        @article{hong2025ita,
        title={ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On},
        author={Hong, Ji Woo and Ton, Tri and Pham, Trung X and Koo, Gwanhyeong and Yoon, Sunjae and Yoo, Chang D},
        journal={arXiv preprint arXiv:2503.20418},
        year={2025}}
    </code></pre>
    </div>
</section>
<!--End Citation -->

<!--Acknowledgements -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <p>This work was supported by Institute for Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2021-II211381, Development of Causal AI through Video Understanding and Reinforcement Learning, and Its Applications to Real Environments) and partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2022-II220184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics)</p>
  </div>
</section>
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
